# API Scraper

![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Symfony](https://img.shields.io/badge/Symfony-000000?style=for-the-badge&logo=symfony&logoColor=white)
![PHP](https://img.shields.io/badge/PHP-777BB4?style=for-the-badge&logo=php&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white)

| Версия             | Ветка                 | Описание                           | Ссылка                                                        |
|--------------------|-----------------------|------------------------------------|---------------------------------------------------------------|
| **Основная**       | `master`              | 1 пост на Message, batch-обработка | текущая страница                                              |


## Оглавление
- [Выбор решения](#обоснование-выбора-решения)
- [Подготовка](#подготовка)
- [Запуск приложения](#запуск)
- [Крон](#крон)
- [TODO-лист](#todo-лист)
- [Замечания](#заметки) (важно!)

Краткое руководство по развертыванию и использованию данного приложения.

## Обоснование выбора решения
Для развертывания и запуска проекта с использованием докера я проанализировал 3 варианта:
1. [Репозиторий](https://github.com/dunglas/symfony-docker) от официальной документации
2. [Скелетон](https://github.com/symfony/skeleton) с минимальным окружением Symfony
3. Кастомная сборка с нуля. Очень похож на второй вариант, различие лишь в композер файле - в скелетоне гораздо
меньше зависимостей. 
--- 
Я выбрал второй вариант - приложение по ТЗ не требует большого функционала, поэтому нет смысла подключать
лишние зависимости, которые мне не нужны (например, веб-интерфейс)

## Подготовка
В настройках PHPStorm необходимо указать пусть к composer.json, чтобы IDE находила все классы
`Settings->PHP->Composer->Path to composer.json`

## Прокси и параллельность
Чтобы все спарсить и уложится в 24 часа, с учетом ограничения в 100 запросов в минуту для IP адреса, я распараллелил
парсинг с использованием отдельных процессов + применение прокси. Прокси есть 100 штук, ограничений по трафику нет, есть
только ограничение по одновременным соединениям - 10 соединений на каждую проксю,
все прокси 100% рабочие. Чтобы запустить скрипт не с моего компа, необходимо написать мне, чтобы я выслал прокси
для конкретного IP.<br><br>
Для удобства распараллеливания воткнул Supervisor в контейнер php-cli. По дефолту запускается 10 воркеров-консюмеров, 
их количество можно изменить в `docker/conf/backend/messenger.conf:9`, так же 7 симфони-процессов для получения
постов из страницы, которые в свою очередь кидают посты в мессенджер. Сообщения обрабатываются батчем по 5 штук с
использованием multiplexing responses.

## Запуск
1. Склонируйте репозиторий ```https://github.com/whatislooooove/api-scraper.git```
2. Соберите и запустите контейнеры 
```bash
  docker compose build
  docker compose up -d
```
3. Подготовьте symfony-окружение:
```bash
  # так как монтирование через volume перезаписывает все что было в контейнере в этой директории
  composer install
  
  # ставим миграции
  php bin/console doctrine:migrations:migrate
```
4. Запустите скрипт 
```bash
  sh startScrate.sh
```

Инициализация процесса сбора происходит одной командой.<br>
Запускаем `startScrate.sh` (можно передать количество процессов во второй параметр) - начнется процесс сбора всех 
постов. <br><br><b>Алгоритм сбора следующий:</b> получаем
по API страницы, в каждой странице 30 постов - каждый пост кидаем в мессенджер с передачей id (супервизор
стартует вместе с контейнером), и у нас получается что одновременно я получаю и список всех постов (с задержкой
в пару секунд, чтобы не было backpressure), и параллельно собираю body для уже 
полученных постов. В базу идет запрос на вставку уже полностью собранного поста с body
5. Ждем, пока спарсится. У меня спарсилось за ~23 часа. Так как в сроки уложился, улучшать алгоритм не стал
(в todo есть заметка)

### Крон
В проекте реализовано получение новых данных через крон задачи (отдельный контейнер, который использует образ основного
сервиса backend) 
- ежедневно в 00:00 проверяется последняя страница в api, если есть новые посты - забираем. Если
постов много (больше 100 страниц - на 100-й странице останавливаемся)
- еженедельно в вс 00:00 полный парсинг всей базы, чтобы полностью синхронизировать локальную базу с удаленной (api)

---

### TODO-лист
1. [ ] 1 - Необходимо разобраться с правами (а именно с пользователями и группами)
2. [x] 2 - Добавить логирование времени - сколько времени занял процесс парсинга
3. [ ] 3 - Поместить external_id после id
4. [ ] 4 - Спарсилось только 449700, а должно 450000. Разобраться, почему так
5. [ ] 5 - Создать отдельную фабрику для ```CreatePostInputDTO```
6. [ ] 6 - Создать функционал для мониторинга парсинга
7. [x] 7 - Переписать хранение прокси. Сделать хранение обычным txt-файлом, чтобы было удобно поменять их на другой машине
8. [x] 8 - Улучшить алгоритм парсинга - на каждый message повесить обработку не 1 поста, а пачки постов (batch), и запросы
кидать асинхронно в stream (начал делать в develop ветке)
9. [ ] 9 - Написать прод-версию докер файла, без монтирования папки с проектом в контейнер (собрать готовый контейнер через
COPY в докер-файле)

---
### Заметки

> **Замечания для себя и проверяющего:**
> - TODO-лист (как здесь, так и в коде) сделал с целью показать слабости системы, о которых я в курсе и о которых 
задумывался, но не сделал, иначе это растянулось бы на очень долго
> - Супервизор работает в контейнере php. Спорный момент
> - Можно было бы добавить еще одну таблицу постов, которая хранила в себе мета информацию (внутренний id, 
время парсинга) и id записи из оригинальной таблицы постов. У постов в апи уже есть id и created_at, было бы неплохо
хранить такие же данные в рамках моей системы
