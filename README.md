# API Scraper

Краткое руководство по развертыванию и использованию данного приложения.

## Обоснование выбора решения
Для развертывания и запуска проекта с использованием докера я проанализировал 3 варианта:
1. [Репозиторий](https://github.com/dunglas/symfony-docker) от официальной документации
2. [Скелетон](https://github.com/symfony/skeleton) с минимальным окружением Symfony
3. Кастомная сборка с нуля. Очень похож на второй вариант, различие лишь в композер файле - в скелетоне гораздо
меньше зависимостей. 
--- 
Я выбрал второй вариант - приложение по ТЗ не требует большого функционала, поэтому нет смысла подключать
лишние зависимости, которые мне не нужны

## Подготовка
В настройках PHPStorm необходимо указать пусть к composer.json, чтобы IDE находила все классы
`Settings->PHP->Composer->Path to composer.json`

## Прокси и параллельность
Чтобы все спарсить и уложится в 24 часа, с учетом ограничения в 100 запросов в минуту для IP адреса, я распараллелил
парсинг с использованием отдельных процессов + применение прокси. Прокси есть 100 штук, ограничений по трафику нет,
все прокси 100% рабочие. Чтобы запустить скрипт не с моего компа, необходимо написать мне, чтобы я выслал прокси
для конкретного IP.<br><br>
Для удобства распараллеливания воткнул Supervisor в контейнер php-cli. По дефолту запускается 15 воркеров-консюмеров, 
их количество можно изменить в `docker/conf/backend/messenger.conf:9`, так же 15 симфони-процессов для получения
постов из страницы, которые в свою очередь кидают посты в мессенджер.

## Запуск
Инициализация процесса сбора происходит одной командой.<br>
Запускаем `php bin/console post:get-list` - начнется процесс сбора всех постов. Алгоритм сбора следующий: получаем
по API страницы, в каждой странице 30 постов - каждый пост кидаем в мессенджер с передачей id (супервизор
стартует вместе с контейнером), и у нас получается что одновременно я получаю и список всех постов, и параллельно 
собираю body для уже полученных постов. В базу идет запрос на вставку уже полностью собранного поста с body. (можно
подумать над оптимизацией, чтобы не делать ~450 000 запросов на insert).

### TODO-лист
1. Необходимо разобраться с правами (а именно с пользователями и группами)
2. Добавить логирование времени - сколько времени занял процесс парсинга
3. Поместить external_id после id
4. Спарсилось только 447000, а должно 450000. Разобраться, почему так
5. Создать отдельную фабрику для ```CreatePostInputDTO```
6. Создать функционал для мониторинга парсинга

### Замечания
Супервизор работает в контейнере php. Спорный момент
