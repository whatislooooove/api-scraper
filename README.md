# API Scraper

Краткое руководство по развертыванию и использованию данного приложения.

## Обоснование выбора решения
Для развертывания и запуска проекта с использованием докера я проанализировал 3 варианта:
1. [Репозиторий](https://github.com/dunglas/symfony-docker) от официальной документации
2. [Скелетон](https://github.com/symfony/skeleton) с минимальным окружением Symfony
3. Кастомная сборка с нуля. Очень похож на второй вариант, различие лишь в композер файле - в скелетоне гораздо
меньше зависимостей. 
--- 
Я выбрал второй вариант - приложение по ТЗ не требует большого функционала, поэтому нет смысла подключать
лишние зависимости, которые мне не нужны (например, веб-интерфейс)

## Подготовка
В настройках PHPStorm необходимо указать пусть к composer.json, чтобы IDE находила все классы
`Settings->PHP->Composer->Path to composer.json`

## Прокси и параллельность
Чтобы все спарсить и уложится в 24 часа, с учетом ограничения в 100 запросов в минуту для IP адреса, я распараллелил
парсинг с использованием отдельных процессов + применение прокси. Прокси есть 100 штук, ограничений по трафику нет,
все прокси 100% рабочие. Чтобы запустить скрипт не с моего компа, необходимо написать мне, чтобы я выслал прокси
для конкретного IP.<br><br>
Для удобства распараллеливания воткнул Supervisor в контейнер php-cli. По дефолту запускается 40 воркеров-консюмеров, 
их количество можно изменить в `docker/conf/backend/messenger.conf:9`, так же 5 симфони-процессов для получения
постов из страницы, которые в свою очередь кидают посты в мессенджер.

## Запуск
1. Склонируйте репозиторий ```https://github.com/whatislooooove/api-scraper.git```
2. Соберите контейнеры ```docker compose build```
3. Запустите контейнеры ```docker compose up -d```.
4. Подготовьте symfony-окружение:
- ```composer install```, так как монтирование через volume перезаписывает все что было в контейнере в этой директории
- ```php bin/console doctrine:migrations:migrate``` - ставим миграции
5. Запустите скрипт ```startScrate.sh```
Инициализация процесса сбора происходит одной командой.<br>
Запускаем `startScrate.sh` (можно передать количество процессов во второй параметр) - начнется процесс сбора всех 
постов. Алгоритм сбора следующий: получаем
по API страницы, в каждой странице 30 постов - каждый пост кидаем в мессенджер с передачей id (супервизор
стартует вместе с контейнером), и у нас получается что одновременно я получаю и список всех постов (и жду, если в
в таблице messages накопилось 10000 записей, чтобы не было backpressure), и параллельно собираю body для уже 
полученных постов. В базу идет запрос на вставку уже полностью собранного поста с body. (можно
подумать над оптимизацией, чтобы не делать ~450 000 запросов на insert).
6. Ждем, пока спарсится. У меня спарсилось за ~23 часа. Так как в сроки уложился, улучшать алгоритм не стал.
(в todo есть заметка)

### TODO-лист
1. Необходимо разобраться с правами (а именно с пользователями и группами)
2. Добавить логирование времени - сколько времени занял процесс парсинга
3. Поместить external_id после id
4. Спарсилось только 447000, а должно 450000. Разобраться, почему так
5. Создать отдельную фабрику для ```CreatePostInputDTO```
6. Создать функционал для мониторинга парсинга
7. Переписать хранение прокси. Сделать хранение обычным txt-файлом, чтобы было удобно поменять их на другой машине
8. Улучшить алгоритм парсинга - на каждый message повесить обработку не 1 поста, а пачки постов (batch), и запросы
кидать асинхронно в stream (начал делать в develop ветке)
9. Написать прод-версию докер файла, без монтирования папки с проектом в контейнер (собрать готовый контейнер через
COPY в докер-файле)

### Замечания для себя и для проверяющего
- Супервизор работает в контейнере php. Спорный момент
- Можно было бы добавить еще одну таблицу постов, которая хранила в себе мета информацию (внутренний id, 
время парсинга) и id записи из оригинальной таблицы постов. У постов в апи уже есть id и created_at, было бы неплохо
хранить такие же данные в рамках моей системы
- TODO-лист(как здесь, так и в коде) сделал с целью показать слабости системы, о которых я в курсе и о которых задумывался, но не сделал,
иначе это растянулось бы на очень долго
